Notes for minishell

These notes describe how I want to build the lexer + parser + executor.


Bash reference: 	https://www.gnu.org/software/bash/manual/bash.html
				or	http://www.opengroup.org/onlinepubs/9699919799/utilities/V3_chap01.html
				or	https://www.computerhope.com/unix/ubash.htm


Contents:

	0.	Preparation
		0.1	Data initialization
		0.2	Printing a prompt
		0.3	Error handling
	
	1.	Reading input
	
	2.	Lexical analysis
		2.1	Scanning characters
		  2.1.1 Categorizing characters
		  2.1.2 Character class macros
		  2.1.3 Character analysis functions
		2.2	Tokenization
		  2.2.1 Structure of a token
		  2.2.2 Token classification
		2.3	Example
	
	3.	Expansions
		3.1	Environment variable expansion
		3.2	Expansion of special variables (to do)
		3.3 Example

	4.	Parser - Abstract syntax tree (AST)


minishell

	0.	Preparation

		Before the shell starts to interpret and execute commands,
		it has to initialize some of its key data structures,
		print a prompt in the terminal and handle errors properly.

		0.1	Data initialization

			-	A pointer to a struct of type "t_data" is used to store any data.

			typedef struct s_data
			{
				char	 **env;		 // a copy of the environment variable list.
				char	 *line;		 // raw input string.
				size_t	 line_len;	 // length of raw input string.
				t_token	 *token;	 // head of linked list of tokens.
				t_token	 *token_ptr; // temp node in token list.
				t_astree *astree;	 // pointer to root node of the AST.
				int		 error;		 // variable for specific error handling.
				int		 token_mask; // bit mask of all token types in the line.
			}	t_data;

			-	The function t_data *init_data(char **envp)
				handles data initialization in the following way:
				-	a pointer to t_data gets created with a call to 
					'ft_calloc(1, sizof(t_data))'
				-	in order to create a copy of the environment list,
					the parameter 'char **envp' is passed to the sub function
					'copy_envp(char **envp)'.
					A deep copy of 'char **envp' is stored in t_data data.env.
				-	the t_data pointer is returned

		0.2	Printing a prompt

			-	a call to print_prompt() prints the shell prompt
			-	with the help of getcwd() the current working directory is printed
			-	the macros BGRN, WHT, BBLU, and END are used to display colors
		
			TODO:
				-	if we have time we can add the username and hostname
					to the prompt string.
					similar to this: "greg@ubuntu:/home/greg$ "

		0.3	Error handling

			-	handling errors is pretty easy, since it is allowed to use
				'errno', 'exit()', 'printf()' and 'strerror()'.
			-	after each call to 'malloc()', 'read()', 'write()' or any
				other libc function, the shell checks the value of the global
				variable 'errno'.
			-	if 'errno' is set to zero, then no error occurred and the shell
				proceeds with the current process.
			-	if 'errno' has a positive value, then an error occurred and
				the shell must exit properly with the value of 'errno'.
			-	'void exit_shell(int error_id)' can be called with 'errno'
				as its first parameter.
				This function prints the error string and error number before
				a call to 'exit(error_id)' ultimately exits the current
				process of the shell.

	1.	Reading input

		Minishell reads user input via a terminal from the standard input.

		-	the function 'size_t read_line(char **line)' reads from stdin
			one byte at a time and stores the data inside its
			first parameter 'char **line'.
		-	'size_t read_line(char **line)' is not meant to be a recreation
			of the libc 'char *readline(const char *)'.
			That's a coincidence that both functions share almost the same name.
		-	every character gets copied up until a newline, with the newline itself.
		-	the total sum of bytes that have been read is returned.

		TODO:
			-	'size_t read_line(char **line)' covers only basic input reading.
				We need to add some command line discipline with termcaps.

	2.	Lexical analysis

		Lexical analysis is the process of interpreting a sequence of characters
		to a list of strings (tokens) with an assigned meaning.

		2.1	Scanning characters

		-	After reading a line from stdin, each character
			of this line gets analyzed one byte at a time.
			Some characters have one or multiple special meanings to the shell.
			According to the char type, the shell decides when to perform
			actions to chop up the raw input line into tokens.

		2.1.1	Categorizing characters
		
			Characters are grouped into these categories:

			break:			a character that indicates the end of a token and the creation of a new token.
			blank:			used to interpret empty blank spaces.
			quote:			marks start or endpoint of quoted strings.
			bsdquote:		chars that have a special meaning when backslashed inside double quotes.
			meta:			shell operators.
			expansion:		triggers variable expansion.
			escape:			escaping char which preserves the literal value of the next character.
			special_var:	chars with special meaning when expanding variables.
			end:			chars which mark the end of command line.

			Char	|	Description		|	Category
			' '		-	single space	-	break/blank
			'\t'	-	tab space		-	break/blank
			'\n'	-	newline			-	break/bsdquote/end
			'\''	-	single quote	-	quote
			'\"'	-	double quote	-	quote/bsdquote
			'<'		-	less than		-	meta/break
			'>'		-	bigger than		-	meta/break
			'|'		-	pipe			-	meta/break
			'$'		-	dollar sign		-	expansion/bsdquote
			'\\'	-	backslash		-	bsdquote/escape
			'`'		-	back quote		-	bsdquote
			'?'		-	question mark	-	special_var
			'\0'	-	null terminator	-	end

		2.1.2	Character class macros
		
			The following macros are used to globally define the different
			char categories with constant strings.
		
			# define META_CHAR			"<>|"
			# define BREAK_CHAR			"<>| \t\n"
			# define QUOTE_CHAR			"\"\'"
			# define EXPANSION_CHAR		"$"
			# define SPECIAL_VAR_CHAR	"?"
			# define BLANK_CHAR			"\t "
			# define END_CHAR			"\n\0"
		
		2.1.3	Character analysis functions
		
			For each character group there is an associated function,
			which takes one character as parameter.
			These functions return a positive value if the given char belongs
			to a group or NULL if the given character is not part of a group.
			
			char		*is_meta(char c);
			char		*is_break(char c);
			char		*is_quote(char c);
			char		*is_expansion(char c);
			char		*is_special_var(char c);
			char		*is_blank(char c);
			char		*is_end(char c);


		2.2	Tokenization

			Tokenization is the process of classifying parts of a raw
			sequence of characters into small units called tokens.
			Tokens can be used by a parser to build a data structure
			that represents the composition of a command.

		
		2.2.1 Structure of a token
		
			-	Tokens are stored inside a linked list.

			typedef struct	s_token
			{
				int				type;	//	token type		(e.g: WORD)
				char			*str;	//	token string	(e.g: "echo")
				struct s_token	*next;	//	pointer to next token
			}				t_token;

		
		2.2.2 Token classification
		
			Tokens are marked with predefined values to classify their meaning.
			These values can be bitwise ORed so that it is possible to mark and
			identify multitype tokens.
			(e.g. "$SHLVL", which is a WORD|DQUOTE|EXPAND)

			# define EMPTY		0x0000 	//	Empty token		""
			# define WORD		0x0001	//	Simple word		"echo"
			# define PIPE		0x0002	//	"|"
			# define HERE_DOC	0x0004	//	"<<"
			# define REDIR_IN	0x0008	//	"<"
			# define APPEND		0x0010	//	">>"
			# define REDIR_OUT	0x0020	//	">"
			# define EXPAND		0x0040	//	"$"
			# define SQUOTE		0x0080	//	"'42'"
			# define DQUOTE		0x0100	//	""42""
		
		2.3	Example
			
			Let's consider the following input line:
			// VAR=42
				echo -n $VAR | grep "1" > 1.txt

			The tokenizer will create this list of tokens:

			token->str		token->type
			"echo"		|	WORD
			"-n"		|	WORD
			"$VAR"		|	WORD|EXPAND
			"|"			|	PIPE
			"grep"		|	WORD
			"1"			|	WORD|DQUOTE
			">"			|	REDIR_OUT
			"1.txt"		|	WORD
			"\n"		|	EMPTY

	3.	Expansions

		-	As stated in the bash manual:
			"Expansion is performed on the command line after it has been split into tokens."
		
		-	Order of expansions:
			1.	Parameter and variable expansion
				->	$?, $SHLVL, $ARG
			2.	Word splitting after expansion
				-> 	Explicit null arguments ("" or '') are retained and
					passed to commands as empty strings.
				-> 	Unquoted implicit null arguments resulting from expansion
					are removed
				->	When a quoted null argument appears as part of a word whose
					expansion is non-null, the null argument is removed.
				->	the word -d'' becomes -d after word splitting and null argument removal.

		3.1	Environment variable expansion

			-	A dollar sign '$' is used to expand variables to their current values
				->	$HOST, $SHELL, $PATH
			-	If a '$' is followed by a non existing variable name
				it is replaced by an empty string
			-	The lexer marks tokens as type EXPAND if they contain a '$' character
				inside their token string
			-	The name of a variable can contain only letters (a to z or A to Z),
				numbers (0 to 9) or the underscore character (_)
				->	$ echo "$SHLVL"		# prints value of variable SHLVL
				->	$ echo "$SHLVL_0"	# prints nothing if "SHLVL_0" doesn't exist
				->	$ echo "$SHLVL="	# prints value of variable SHLVL and appends a '='
			-	Variables can also be expanded from within other variables
				->	$ export ARG="$PWD, $SHELL, $SHLVL"; echo $ARG
					# Output: "/home, /bin/bash, 1"
			
			Steps how minishell expands variables:

			1.	Loop through token string:	"abc $PWD 123" where PWD=/home
			2.	If current char is '$' then get the variable name "PWD"
			3.	Get the value of variable name "PWD=/home" and duplicate the string
				on the heap.
			4.	Save those substrings within a linked list:
				-	"abc "
				-	"/home"
				-	"123"
			5.	Merge substrings into one string
				-	"abc /home 123"
			6.	Free old token string and replace it with the new string
				"abc $PWD 123" -> "abc /home 123"
			7.	Check if the new string contains new expansions.
			8.	Do word splitting if necessary.

		3.2 Expansion of special variables (to do)

		3.3 Example

		// VAR=42
		echo -n $VAR | grep "1" > 1.txt

			The resulting list of tokens after variable and
			path expansion:

			token->str			token->type
			"echo"			|	WORD
			"-n"			|	WORD
			"42"			|	WORD|EXPAND
			"|"				|	PIPE
			"/usr/bin/grep"	|	WORD
			"1"				|	WORD|DQUOTE
			">"				|	REDIR_OUT
			"1.txt"			|	WORD
			"\n"			|	EMPTY
	
	4.	Parser - Abstract syntax tree (AST)

		-	The previously created tokens will be parsed into an AST.

		-	The AST is a binary tree-like data structure which contains 
			the hierarchy of the commands.

		4.1	Shell grammar
		
			We will use the following grammar, which is based on real bash:

			<word>			::=	WORD		# A <word> is basically a token which has the WORD bit inside his bitmask

			<word list>		::=	<word>
							|	<word list> <word>

			<filename>		::=	<word>

			<redirection>	::=	'<' <filename> <word_list>	# redir_in
        					|	<word_list> '<' <filename>
							|	'<' <filename> 
        					|
							|	'>' <filename> <word_list>	# redir_out
        					|	<word_list> '>' <filename>
							|	'>' <filename>
        					|
        					|	'>>' <filename> <word_list>	# append
        					|	<word_list> '>>' <filename>
							|	'>>' <filename>
							|
        					|	'<<' <filename> <word_list>	# here_docs
        					|	<word_list> '<<' <filename>
							|	'<<' <filename>
							|	
							|	# Add later
        					|	<word_list> '<>' <word_list>
        					|	'<>' <word_list>
        					|	'>|' <word_list>
        					|	<word_list> '>|' <word_list>

		<redirection_list>	::=		<redirection>
							|		<redirection_list> <redirection>

			<command>		::=		<word_list>
							|		<redirection>

			<pipeline>		::=		<command>
							|		<pipeline> '|' <command>



		-	tokens are parsed after these grammar rules

		-	for each grammar rule there is a function which checks
			the constellation of tokens recursively.

		4.2	AST struct
		
			-	each node in the tree will be represented by this struct:

			typedef struct s_astree
			{
				struct s_astree	*left;
				struct s_astree	*right;
				char			*str;
				int				type;
			}	t_astree;

		4.3	Classifying AST nodes
	
		-	node types can be ORed with the following values:
			
		# define AST_PIPE		0x0001
		# define AST_HERE_DOC	0x0002
		# define AST_REDIR_IN	0x0004
		# define AST_APPEND		0x0008
		# define AST_REDIR_OUT	0x0010
		# define AST_WORD		0x0020

		4.4	Example
			
			Let's consider the following input line:
			// VAR=42
				echo -n $VAR | grep "1" > 1.txt

			The resulting list of tokens after variable and
			path expansion:

			token->str			token->type
			"echo"			|	WORD
			"-n"			|	WORD
			"42"			|	WORD
			"|"				|	PIPE
			"/usr/bin/grep"	|	WORD
			"1"				|	WORD|DQUOTE
			">"				|	REDIR_OUT
			"1.txt"			|	WORD
			"\n"			|	EMPTY

			AST:

              	    ______AST_PIPE_______
                   /		           \
                  /			            \
			   __/			             \__
	    	WORD,                   	AST_REDIR_OUT,
		   "echo"			                "1.txt"
		 /        \                        /        \
		/          \                      /          \
	   /            \                    /            \
	NULL    	WORD,         			NULL         WORD,
		        "-n"                             "/usr/bin/grep"       
		      /       \                        	      /     \
			 /         \                       	     /       \
			/	        \                      	    /         \
		  NULL    	  WORD,         			 NULL    	 WORD,
			  	      "42"                     	              "1"
			          /  \							          /  \
				     /    \							         /    \
			        /      \						        /      \
                  NULL    NULL						      NULL    NULL


	5.	Command execution	-	still to do

		-	While reading the AST we will use the following data structure
			to build the command arguments which we will later pass to execve():
		
			typedef struct s_command
			{
				int argc;
				char **argv;
				/**
				*	maybe add some more variables
				*	for storing pipe fd's
				*	and redirections
				**/
			}	t_command;

			// Example: echo "42" "21"

			t_command	*cmd;

			cmd->argc = 3;
			cmd->argv = {"echo", "42", "21", NULL};

		-	The executor will start to read the AST from the root node.
			The root node will decide how the commands will be executed.

		Let's reconsider the example from step 3:

			AST:

              	    ______AST_PIPE_______
                   /		           \
                  /			            \
			   __/			             \__
	    	WORD,                   	AST_REDIR_OUT,
		   "echo"			                "1.txt"
		 /        \                        /        \
		/          \                      /          \
	   /            \                    /            \
	NULL    	WORD,         			NULL         WORD,
		        "-n"                             "/usr/bin/grep"       
		      /       \                        	      /     \
			 /         \                       	     /       \
			/	        \                      	    /         \
		  NULL    	  WORD,         			 NULL    	 WORD,
			  	      "42"                     	              "1"
			          /  \							          /  \
				     /    \							         /    \
			        /      \						        /      \
                  NULL    NULL						      NULL    NULL
		
		-	PIPE is the root node so we need to call a function
			which creates a pipe with the
			simple command 'echo -n 42' as the write end,
			and the redirection job '/usr/bin/grep "1" > 1.txt' as the read end.

		-	So we need to follow these steps:
			1.	Create pipe
			2.	Execute simple command 'echo -n 42'
				->	pipe write end
			3.	Execute simple command with redirection 'grep "1" > 1.txt'
				->	pipe read end



weird inputs:

echo "abc
123
456
789
def
" > file

ls | grep "o" | wc -l > file.out < file.in 2> errfile
cat -e | wc -c < file 
cat -e < infile | wc | cat -e > out
cat -e < file | sort | wc -l
cat -e < file.in | sort | wc -l > file.out
> file | grep "l"
> file lol | grep "l"
wc -l < file
wc -l < file > file2
cat -e < file lol
echo "abc" > file1 > file2 > file3	# file1 and file2 are created and file3 gets written into
cat -e < file1 < file2 < file3
wc -l < file.in > file.out
grep "a" << LOL ABC		# "ABC" gets passed as 2nd argument to grep after it got its input from the here doc
echo > file file1
echo "$ARG"; echo $ARG	# word splitting in unquoted argument ARG="-n abc"
$ARG; "$ARG" # ARG="echo abc"
