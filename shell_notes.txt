Notes for minishell

These notes describe how I want to build the lexer + parser + executor.


Bash reference: 	https://www.gnu.org/software/bash/manual/bash.html#Shell-Expansions
				or	http://www.opengroup.org/onlinepubs/9699919799/utilities/V3_chap01.html
				or	https://www.computerhope.com/unix/ubash.htm


minishell

	0.	Preparation

		0.1	Data initialization

			-	A pointer to a struct of type "t_data" is used to store all data which
				will be used for the process of command line interpretation.
			-	The function t_data *init_data(char **envp) handles the data initialization
				in the following way:
				-	a pointer to t_data gets created with a call to ft_calloc(1, sizof(t_data))
				-	in order to create a copy of the environment list the parameter char **envp
					is passed to the subfunction copy_envp(char **envp),
					where a deep copy of char **envp is created and stored at t_data.env.
				-	the t_data pointer with the environment list and all other members set to zero
					is returned.
			TODO:
				-	make a copy of the environment outside of init_data() for modularity
				-	store environment inside a linked list instead of a 2d array for simpler
					variable modification.

		0.2	Printing a prompt

			-	a call to print_prompt() prints the shell prompt
			-	with the help of getcwd() the current working directory is printed
			-	the macros BGRN, WHT, BBLU, and END are used to display colors
			
			TODO:
				-	if we have time we can add the username and hostname to the prompt string
					similar to this: "greg@ubuntu:/home/greg$ "


	1.	Reading input

		-	the function size_t read_line(char **line) reads from stdin one byte at a time
			and stores the data inside its first parameter 'char **line'.
		-	size_t read_line(char **) is not meant to be a recreation
			of the libc char *readline(const char *).
			That's a coincidence that both functions share almost the same name.
		-	every character gets copied up until a newline, and the newline itself.
		-	the total sum of bytes that have been read is returned.

		TODO:
			-	size_t read_line(char **) covers only basic input reading so we need to
				add some command line discipline with termcaps.

	2.	Lexical analysis

		2.1	Tokenize user input

		-	In order to process user input into commands
			minishell splits the the input line into tokens.
			All tokens will be stored inside a linked list of token structs.

			typedef struct	s_token
			{
				int				type;	//	token type		(e.g: WORD)
				char			*str;	//	token string	(e.g: "echo")
				struct s_token	*next;	//	pointer to next token
			}				t_token;

		-	Each character inside the input line gets analyzed one byte at a time.

		-	Characters are grouped into different categories:

			break:			indicates when to break a line into a new token.
			blank:			used to interpret empty blanks.
			quote:			marks start or endpoint of quotation.
			bsdquote:		chars that have a special meaning when backslashed in double quotes.
			meta:			shell operators.
			expansion:		triggers variable expansion.
			escape:			a char which preserves the literal value of the next character.
			special_var:	chars with special meaning for variable expansion.
			end:			command line terminators.

			Char	|	Description		|	Category
			' '		-	single space	-	break/blank
			'\t'	-	tab space		-	break/blank
			'\n'	-	newline			-	break/bsdquote/end
			';'		-	semicolon		-	meta/break
			'\''	-	single quote	-	quote
			'\"'	-	double quote	-	quote/bsdquote
			'<'		-	less than		-	meta/break
			'>'		-	bigger than		-	meta/break
			'|'		-	pipe			-	meta/break
			'$'		-	dollar sign		-	expansion/bsdquote
			'\\'	-	backslash		-	bsdquote/escape
			'`'		-	back quote		-	bsdquote
			'?'		-	question mark	-	special_var
			'\0'	-	null terminator	-	end
		
		-	Tokens are marked with defined values to distinguish their purpose.
			These values can be bitwise ORed so that it is possible to mark and
			identify multitype tokens. (e.g. "$SHLVL", which is a WORD|DQUOTE|EXPAND)

			# define EMPTY		0x0000	//	Empty token		""
			# define WORD		0x0001	//	Simple word		"echo"
			# define SEMICOLON	0x0002	//	";"
			# define PIPE		0x0004	//	"|"
			# define REDIR_IN	0x0008	//	"<"
			# define REDIR_OUT	0x0010	//	">"
			# define APPEND		0x0020	//	">>"
			# define EXPAND		0x0040	//	"$"
			# define SQUOTE		0x0080	//	"'42'"
			# define DQUOTE		0x0100	//	""42""

		-	Basic steps of the tokenizer:

			1.	Start iterating through input line with an index variable i = 0.

			2.	If line[i] has no special meaning then copy the character into the token string.
				->	token->str[j] = line[i]
				->	j++;
				->	i++;

			3.	If line[i] is a break ("<>;| \t\n") or a quote ("\"\'") and the current
				token string is not empty then create a new empty token.
				(token strings are allocated with ft_calloc so null termination is not needed)
				->	token->next = ft_calloc(1, sizeof(t_token));
				->	token = token->next;
				->	token->str = ft_calloc(strlen(line) - i, sizeof(char));

			4.	If line[i] is a meta char ("<>;|") then make a token only containing the meta char.
				Mark the token type depending on the current meta char.
				Then initialize a new empty token.
				->	token->str[0] = line[i]
				->	token->next = ft_calloc(1, sizeof(t_token));
				->	token = token->next;
				->	token->str = ft_calloc(strlen(line) - i, sizeof(char));
			
			5.	Quoted input starting with '\"' or '\'' gets tokenized as follows:
				5.1	If line[i] is a single quote '\'' all characters until the
					next single quote are copied into the token string.
					If the quotes are empty '' then return.
					If end of line was reached and no closing single quote was found,
					the input is seen as a syntax error.
				5.2	If line[i] is a double quote '\"' the same instructions are done like
					processing single quotes.
					But it is additionally checked for escape characters and variable expansions.

			6.	Basic syntax checks are currently present within the tokenizer.
				Deep syntax checking will happen at a later stage.

				Basic syntax checks include:
					-	multiple semicolons "echo ;;"
					-	open quotes	"echo 'abc "
					-	invalid operators "echo >>>"

		-	Let's consider the following input line:
			// VAR=42
				echo "42 is $VAR" -n | grep "1" < 1.txt

			The tokenizer will create this list of tokens:

			token->str		token->type
			"echo"		|	WORD
			"42 is $VAR"|	WORD|EXPAND|DQUOTE
			"-n"		|	WORD
			"|"			|	PIPE
			"grep"		|	WORD
			"1"			|	WORD|DQUOTE
			"<"			|	REDIR_IN
			"1.txt"		|	WORD
			"\n"		|	EMPTY


		2.2	Shell Expansions	-	in progress
			
			-	As stated in the bash manual:
				"Expansion is performed on the command line after it has been split into tokens."
			
			-	Order of expansions:
				1.	Parameter and variable expansion
					->	$?, $SHLVL, $ARG, "$ARG"
				2.	Word splitting after expansion
					-> 	Explicit null arguments ("" or '') are retained and
						passed to commands as empty strings.
					-> 	Unquoted implicit null arguments resulting from expansion
						are removed
					-> 	If a parameter with no value is expanded within double quotes,
						a null argument results and passed to command as empty string
					->	When a quoted null argument appears as part of a word whose
						expansion is non-null, the null argument is removed.
					->	the word -d'' becomes -d after word splitting and null argument removal.

			After expanding the pathnames and variables the token list 
			will look like this:

			// VAR=42
				echo "42 is $VAR" -n | grep "1" < 1.txt

			token->str			token->type
			"echo"			|	WORD
			"42 is 42"		|	WORD|EXPAND|DQUOTE	<--- Variable expansion
			"-n"			|	WORD
			"|"				|	PIPE
			"/usr/bin/grep"	|	WORD				<--- Pathname expansion
			"1"				|	WORD|DQUOTE
			"<"				|	REDIR_IN
			"1.txt"			|	WORD
			"\n"			|	EMPTY
		
		2.3	To do
				-	Expand variables 'ARG1=42; echo $ARG1' -> 'echo 42'
				-	Expand pathnames 'cat file.txt' -> '/usr/bin/cat file.txt'
				-	Variable assignment ???
					->	info env
	
	3.	Build an abstract syntax tree (AST)	-	most of code done - need more tests

		-	The previously created tokens will be parsed into an AST.

		-	The AST is a binary tree-like data structure which contains 
			the hierarchy of the commands.

		-	We will use the following grammar, which is based on real bash:

			<command line>	::= 	<job> ';' <command line>
								|	<job> ';'
									<job>

			<job>			::=		<command> '|' <job>
								|	<command>

			<command>		::=		<simple command> '<' <filename>
								|	<simple command> '>' <filename>
								|	<simple command>

			<simple command>::=		<pathname> <token list>

			<token list>	::=		<token> <token list>
								|	(EMPTY)

		-	tokens are parsed after these grammar rules

		-	for each grammar rule there is function which checks
			the constellation of tokens recursively.

		-	each node in the tree will be represented by this struct:

			typedef struct s_astree
			{
				struct s_astree	*left;
				struct s_astree	*right;
				char			*str;
				int				type;
			}	t_astree;

		-	node types can have the folling values:
			
			AST_PIPE		=	1		//	<command> '|' <job>
			AST_LIST		=	2		//	<job> ';' <command line>
			AST_REDIR_IN	=	4		//	<simple command> '<' <filename>
			AST_REDIR_OUT	=	8		//	<simple command> '>' <filename>
			AST_CMD			=	16		//	<pathname> <token list>
			AST_CMD_ARG		=	32		//	<token> <token list
			AST_TOKEN		=	64		//	<token>

		-	Example:
			Input: echo "42 is $VAR" -n | grep "1" < 1.txt	# VAR equals 42
			Tokens (after expansion):

				token->str			token->type
				"echo"			|	WORD
				"42 is 42"		|	WORD|EXPAND|DQUOTE
				"-n"			|	WORD
				"|"				|	PIPE
				"/usr/bin/grep"	|	WORD
				"1"				|	WORD|DQUOTE
				"<"				|	REDIR_IN
				"1.txt"			|	WORD
				"\n"			|	EMPTY

			AST:

              	    ______AST_PIPE_______
                   /		           \
                  /			            \
			   __/			             \__
	    WORD|AST_CMD,                   WORD|AST_REDIR_IN, 
		   "echo"			                "1.txt"
		 /        \                        /        \
		/          \                      /          \
	   /            \                    /            \
	NULL    WORD|AST_CMD_ARG,         NULL         WORD|AST_CMD,
		     "42 is 42"                           "/usr/bin/grep"       
		      /       \                        	      /     \
			 /         \                       	     /       \
			/	        \                      	    /         \
		  NULL    WORD|AST_CMD_ARG,         	 NULL    WORD|AST_CMD_ARG,
			  	      "-n"                     	              "1"
			          /  \							          /  \
				     /    \							         /    \
			        /      \						        /      \
                  NULL    NULL						      NULL    NULL


	
	4.	Execute AST nodes	-	still to do

		-	While reading the AST we will use the following data structure
			to build the command arguments which we will later pass to execve():
		
			typedef struct s_command
			{
				int argc;
				char **argv;
				/**
				*	maybe add some more variables
				*	for storing pipe fd's
				*	and redirections
				**/
			}	t_command;

			// Example: echo "42" "21"

			t_command	*cmd;

			cmd->argc = 3;
			cmd->argv = {"echo", "42", "21", NULL};

		-	The executor will start to read the AST from the root node.
			The root node will decide how the commands will be executed.

		Let's reconsider the example from step 3:

			AST:

              	    ______AST_PIPE_______
                   /		           \
                  /			            \
			   __/			             \__
	    WORD|AST_CMD,                   WORD|AST_REDIR_IN, 
		   "echo"			                "1.txt"
		 /        \                        /        \
		/          \                      /          \
	   /            \                    /            \
	NULL    WORD|AST_CMD_ARG,         NULL         WORD|AST_CMD,
		     "42 is 42"                           "/usr/bin/grep"       
		      /       \                        	      /     \
			 /         \                       	     /       \
			/	        \                      	    /         \
		  NULL    WORD|AST_CMD_ARG,              NULL    WORD|AST_CMD_ARG,
			  	      "-n"                     	              "1"
			          /  \							          /  \
				     /    \							         /    \
			        /      \						        /      \
                  NULL    NULL						      NULL    NULL
		
		-	PIPE is the root node so we need to call a function
			which creates a pipe with the
			simple command 'echo "42 is 42" -n' as the write end,
			and the redirection job 'grep "1" < 1.txt' as the read end.

		-	So we need to follow these steps:
			1.	Create pipe
			2.	Execute simple command 'echo "42 is 42" -n'
				->	pipe write end
			3.	Execute simple command with redirection 'grep "1" < 1.txt'
				->	pipe read end



weird inputs:

ls | grep "o" | wc -l > file.out < file.in > errfile
